import numpy as np

# Определяем градиент функции
def gradient_f(x):
    x1, x2 = x
    df_dx1 = 1 + x1 / np.sqrt(1 + x1**2 + x2**2)
    df_dx2 = 2 + 2*x2 / np.sqrt(1 + x1**2 + x2**2)
    return np.array([df_dx1, df_dx2])

# Определяем функцию f(x)
def f(x):
    x1, x2 = x
    return x1 + 2*x2 + 4*np.sqrt(1 + x1**2 + x2**2)

# Реализуем метод наискорейшего спуска
def gradient_descent(f, gradient_f, initial_x, learning_rate, tolerance, max_iterations):
    x = initial_x
    for _ in range(max_iterations):
        grad = gradient_f(x)
        next_x = x - learning_rate * grad
        if np.linalg.norm(next_x - x) < tolerance:
            break
        x = next_x
    return x

# Задаем начальное значение, скорость обучения (learning rate), допустимую погрешность и максимальное количество итераций
initial_x = np.array([0.0, 0.0])
learning_rate = 0.1
tolerance = 1e-5
max_iterations = 1000

# Применяем метод наискорейшего спуска для поиска оптимума
optimal_x = gradient_descent(f, gradient_f, initial_x, learning_rate, tolerance, max_iterations)
optimal_value = f(optimal_x)

print("Оптимальное значение x:", optimal_x)
print("Значение функции в оптимальной точке:", optimal_value)
